[["index.html", "statistics for social science Chapter 1 Introduction", " statistics for social science Milin Zhang 2026-01-09 Chapter 1 Introduction Goal of this site: organize my own statistical notes mainly focusing on application for social science note down new statistical concepts when reading research papers and how they can contribute to my research which highly relies on regression analysis and longitudinal data analysis to test hypothesis The notes taken on this site are rarely my own content, i mainly take notes from following sources: MA course “Multivariate Statistics” handouts by Maud Reveilhac: https://meth.ikmz.uzh.ch/ Textbooks series of “Discovering Statistics” by Andy Field: https://discoveringstatistics.com/ Statistics Tutorial by numiqo: https://numiqo.com/tutorial/get-started question marks what does “Locally weighted scatterplot smoothing (LOWESS) trend lines with alpha = .2.” mean? "],["basic-concepts.html", "Chapter 2 Basic Concepts 2.1 generating theories and test them 2.2 level of measurement 2.3 validity and reliability 2.4 correlational and experimental research 2.5 descriptive data analysis", " Chapter 2 Basic Concepts 2.1 generating theories and test them 2.1.1 Theory general prediction ans statements from logical deduction, experience and observations, e.g., Social role theory is a concept in sociology and social psychology that explains how individuals behave based on the roles they occupy in society, such as parent, teacher, or leader. These roles come with specific expectations and norms that guide behavior in various contexts. 2.1.2 hypothesis scientific statements that are produced based on theory, which are generally measurable and testable. e..g, from social role theory, we predict that women, following their gender role, are less likely to apply for management role. reject of the hypothesis: falsification. source: discovering statistics with R, 1.4 2.2 level of measurement categorical variable e.g., names of species: human, dog, cat binary variable yes, no 1, 0 nominal var e.g., use number to denote categories (multiple possibilities) ordinal var logically ordered categories, e.g., gold, silver, bronze medal continuous var interval var 5-point likert scale ratio var e.g., twice as much helpful, etc must have a valid, meaningful 0 point discreet var where one can only take up certain values (e.g., only 1-5 in a 5-point likert scale), not every numerical value possible source: discovering statistics with R, 1.5.1.2 2.3 validity and reliability (criterion) Validity if an instrument measures what is designs to measure e.g., is this survey question fully capturing people’s amount of DMU? content validity the degree to which individual items represent the construct being measured, and cover the full range of the construct Reliability the ability of the measure to produce the same results under the same conditions (replicability / robustness?) source: discovering statistics with R, 1.5.3 2.4 correlational and experimental research 2.4.1 correlational / cross-sectional research notion: observe what naturally happens in the world without directly interfering with it, typically work on the phenomena that we cannot easily manipulate caveat: problem of proving causal relationship stat method: typically measured with regression, correlation 2.4.2 experimental notion: manipulate certain variable and observe the effect on another stat method: typically measured with ANOVA, t-test But mathematically , the statistical methods mentioned above are identical two types of data collection between group / between subject design: different group of participants take part in each experimental condition within subject / repeated measurement design: different experimental condition on the same group of participants two types of variation systematic: created by specific experimental manipulation, e.g., doing sth to all participants in one condition but not in the other condition unsystematic: random factors that exist between the experimental conditions (e.g., group character difference, time of the day) -&gt; try to minimize this radomization counterbalance the systematic variation e.g., randomize the order of the conditions to counterbalance the practice effect source: discovering statistics with R, 1.6.1. + 1.6.2. 2.5 descriptive data analysis 2.5.1 frequency distribution normal distribution skew value and kurtosis value are both 0 in this case, mean is 0, sd is 1 distortion from normal distribution (plotting out histogram) positively skewed (left leaning) and negatively skewed (right leaning) positive kurtosis (excessively pointy) and negative kurtosis 2.5.2 center of a distribution mean, mode, and the median 2.5.3 dispersion in a distribution range: max minus min quartiles lower quartile second quartile: median upper quartile interquartile: middle 50% of the data a common exclusion strategy: keep the interquartile for the analysis, so to exclude extreme scores 2.5.4 extrapolate to probability (1.7.4) researchers have calculated the probability of certain score occurring if they are fitting a perfect normal distribution if the data we are using is not normally distributed, we can calculate a z-score and calculate the corresponding probability z-score: A z-score, or standard score, indicates how many standard deviations a raw score is from the mean of a population. It is calculated by subtracting the population mean from the raw score and dividing by the population standard deviation. select the raw score calculate z-score (equation in 1.7.4) check in the existing table: the Smaller Portion -&gt; that’s the probability of the value to occur :) source: discovering statistics with R, 1.7. "],["statistical-models.html", "Chapter 3 Statistical Models 3.1 simple statistic model 3.2 going beyond (sample) data 3.3 test our hypothesis with stats", " Chapter 3 Statistical Models model fit we want our model to represent our data collected as well as possible to make our predictions valid. types of models linear model regression, ANOVA both fall under this category non-linear model advice always plot your data first to observe if there is a linear tendency 2.2 3.1 simple statistic model 3.1.1 mean …is a very simple statistical model 3.1.2 model fit … is how well does a model fit our sample data deviance the absolute difference between the model (e.g., mean) and the true data total error sum of all the deviances deviation / sum of squared errors (SS) another measure of model accuracy but is dependent on the amount of data we have variance commonly used, SS / df (degree of freedom) df (degree of freedom) number of observations that are free to vary. so in a sample of N, and we try to hold one parameter constant, e.g., the mean, and once we freely vary N - 1 number of values in our sample, the one that’s left must be a fixed number to hold the parameter of mean constant, then the df of this sample is N -1. standard deviation square root of the variance relation to distribution larger standard deviation, the more the data deviate from the mean, and the flatter the distribution looks like coefficient of variance (CV) … is defined as the ratio of the standard deviation σ to the mean μ 2.4 3.2 going beyond (sample) data … to see if the sample is a good representation of the population ### standard error - population mean - sample mean - mean of a certain sample - sampling variation - different sample drawn with different criterion will create different means - sampling distribution - the distribution of means collected from different sample from the sample population - standard error of the mean (SE) - standard deviation (see [[2.4 Simple statistic model#Model fit]]) of the mean 3.2.1 calculation of SE we need to rely on an approximation of the SE. central limit theorem when sample size gets large (&gt; 30), the sampling distribution has a normal distribution with a mean equal to the population mean, and a standard deviation of: - standard deviation of a sample of data / square root of the sample size can be used to approximate SE when sample size is small (&lt; 30), the sampling distribution has a t-distribution 3.2.2 confidence interval definition the range of values within which we think the population value will fall 95% CI of the Pearson’s r: we are 95% sure that the true value lies within this range (the r will be the median / middle point of this range) calculation background z-score: standard scores from a normal distribution with a mean of 0 and a standard deviation of 1 95% of the z-scores lie within -1.96 to 1.96 -&gt; i.e., in normally distributed data, the confidence interval of the mean is CI [-1.96, 1.96] in the case of non-normal distributed data, we can use calculation (equation 2.1) to center the data back to the mean of 0 and then calculate confidence interval of our data (with reference to z-score in normal distribution) Lower bound of 95% CI: mean - (1.96 x SE) Upper bound of 95% CI: mean + (1.96 x SE) if 99% CI, z.score is -2.58 and 2.58 if the mean distribution represents the true mean well, it should have a small CI 2.5 3.3 test our hypothesis with stats 3.3.1 five stages of research generate RQ through an initial observation generate a theory to explain the initial observation generate hypothesis: from theory to testable prediction collect data to test theory analysis data: fit a statistical model to the data -&gt; test the hypothesis 3.3.2 test statistics … is the ratio of systematic variances to unsystematic variance / effect to error significance and importance a variable’s effect is significant does not mean that the effect is important ==TBA== 2.6 "],["statistical-assumptions.html", "Chapter 4 Statistical assumptions 4.1 Assumption of parametric data 4.2 The assumption of normality 4.3 The assumption of homogeneity of variance 4.4 Correcting problems in the data", " Chapter 4 Statistical assumptions Fantastic assumptions and how to test them :) 4.1 Assumption of parametric data Most of the statistical tests in the following chapters are parametric tests based on normal distribution. In order to draw meaningful results from parametric tests, certain assumptions have to be met. Normally distributed data Homogeneity of variance Interval data To say that data are interval, we must be certain that equal intervals on the scale represent equal differences in the property being measured. (e.g., Likert scale) Independence e.g., data from each participants are independent in a sense that the behaviour of one participant does not influence the behaviour of another. In regression, this assumption is manifested with the error in regression model being uncorrelated. In this chapter we focus on the the normal distribution and homogeneity of variance. 4.2 The assumption of normality … is important for many statistical tests, such as t-test and regression analysis. recap: normal distribution of a sample can be extrapolated to approximate normal distribution of the sampling distribution (which is usually what actually matters) 4.2.1 observing normality plot histogram plot Q-Q plot as in quantile-quantile plot, a quantile is the proportion of cases we find below a certain value. Normally distributed data should be plotted as a straight diagonal line. ggplot2::qplot(sample = df$var, stat = &quot;qq&quot;) 4.2.2 quantifying normality skew negative values: right-skewed positive values: left-skewed kurtosis negative values: flatly distributed positive values: pointedly distributed psych::describe(sample = df$var) ## hacks: run function on different groups by(data = df, INDICES = df$grouping_var, FUN = function_you_want_to_apply) 4.2.3 testing normality Shapiro-Wilk test … is a test that compares the targeted sample to a normally distributed scores with the same mean and SD. Therefore, if a Shapiro-Wilk test is non-significant, it means that our sample is not significantly different from a normally distributed counterpart. If significant, it tells us that our data is potentially not normally distributed. Shapiro-Wilk test is easily turning significant when sample size gets large. Always plot your data to corroborate your findings. stat::shapiro.test(df&amp;var) 4.3 The assumption of homogeneity of variance … means that the variances should be the same through out the data. In design in which you test several groups of participants, this assumption means that each of these samples comes from populations with the same variance. (recap: variance is the sum of square errors divided by degree of freedom) For example, we collected participants’ scoring on different climbing gyms at different time points, the spread of the scores around the mean at each level (aka at each time point) should be roughly the same. 4.3.1 Testing homogeneity of variance to see if variance is stable within each level/group of the variable. Levene’s test … tests the null hypothesis that variances in different groups are equal. If the Levene’s test turns out to be insignificant, the variances are roughly equal between levels, the assumption is tenable. car::leveneTest(df&amp;var, # e.g., the scores for climbing gym df$group, # e.g., different time points center = median / mean) Hartley’s F max: the variance ratio To mitigate the diviation caused by large sample size, it’s worth to check the variance ratio as well, which is the ratio of variances between the group with the biggest variance and the group with the smallest variance, then the ratio is compared to critical values published by Hartley. # option 1 vartest::hartley.test(formula, data, size = &quot;mean&quot;, alpha = 0.05, na.rm = TRUE, verbose = TRUE) # option 2 PMCMRplus::hartleyTest() 4.4 Correcting problems in the data Once the outliers have been spotted, there are 3 options: Remove the case. (but only if you have good reason to believe that they don’t fit in the sample/population) Transform the data -&gt; reduce skewness Change the score (see more on the section 5.8.1) next highest score plus one convert back from a z-score the mean plus two standard deviation 4.4.1 Transforming data Good to know: transforming data as whole won’t change the relation between two variables (e.g., regression), but it does change the difference between variables (e.g., descriptive data). Log transformation (commonly applied) Definition: Take the logarithm of all numbers. However log transformation does not work on 0 and negative values. Try to transform them before logging them out. Targeted case: positive skew, unequal variances How to work in R: df$logvar &lt;- log(df$var) # the merits of adding 1 df$logvar &lt;- log(df$var + 1) # so to avoid having ones that scored 1 become 0 after the log transformation Square root transformation Definition: Take the square root of all numbers. But the same problem with log tranform persists. Targeted case: positive skew, unequal variances How to work in R: df$sqrt_var &lt;- sqrt(df$var) Reciprocal transformation Definition: dividing 1 by each score. However it change the sequece of the data, to mitigate such effect, you can change the score by minusing the highest score to each score first. Targeted case: positive skew, unequal variances Reverse score transformation Definition: Any transformation above can deal with negative skew, but you need to reverse the score first. (more in section 5.8.1) Targeted case: negative skew 4.4.2 If everything is still horribly wrong try to choose another method that does not require these assumptions to be met robust method and bootstrap … to be added, section 5.8.4 "],["correlation.html", "Chapter 5 Correlation 5.1 How do we measure relationships 5.2 Bivariate correlation 5.3 Partial correlation 5.4 Comparing correlations 5.5 Computing effect size and reporting results", " Chapter 5 Correlation 5.1 How do we measure relationships 5.1.1 Defining covariance Variance -&gt; square errors divided by degree of freedom Covariance Essentially, an expression of the relationship between variables, i.e., the relatedness of two variables, i.e., the extent of similarity between patterns of differences of the two variables. is the cross-product deviations (multiplying the deviations of one variable by the corresponding deviation of a second variable) divided by degree of freedom. However, the scale of covariance is dependent on the scale of measurement. Therefore we need a process of standardization to set the measurements to the same unit, therefore the numbers are comparable. 5.1.2 Standardization and the correlation efficient Standardization Conversion of a value that is dependent on the measurement scale into a standard set of units. To standardize the deviation of the mean from one variable, we can divide the observed deviation by the standard deviation. recap: standard deviation is the square root of the variance. When there are two variables and we want to standardize the covariance, we can first multiply the two standard deviation of the two variables, and then divide the covariance by the multiplied product. This value is known as r, the correlation coefficient. Correlation coefficient aka Pearson product-moment correlation coefficient, or Pearson correlation coefficient Value lies within -1 and 1. … is an expression of standardized the measure of an observed effect. Commonly, values of ±.1 -&gt; small effect, ±.3 -&gt; medium effect, and ±.5 is a large effect. 5.1.3 Significance of the correlation coefficient To test the hypothesis that the observed coefficient is different from zero We need to transform our r to z.score to observe it in a normal distribution 5.1.4 Confidence intervals for r 5.1.5 Warning for concluding to causality We cannot assume causality just for the existence of a significant correlation coefficient. The third-variable problem The unknown direction of causality 5.2 Bivariate correlation … us a cirrrelation between two variables, including 5.2.1 Pearson’s r Assumptions: Interval data If one wants to prove significance, the sampling distributions need to be normally distributed, that is, our sample data are normally distributed (or if we have a large sample) . -&gt; I’m practically lost at this sentence so I asked AI: The “Sampling Distribution” (The Hypothetical Universe) When you calculate Pearson’s r, you get one number (e.g., r=0.5). But to “prove significance,” a computer asks: “If there was actually zero relationship in the real world, how likely is it that I’d accidentally stumble upon an r=0.5 just by luck?” To answer this, it imagines a sampling distribution—a bell curve of every possible r value you could have gotten from thousands of random samples. The Problem: The math used to calculate your p-value (the t-test) assumes this “hypothetical universe” of results follows a specific, predictable shape (a t-distribution). The Link: If your sample data is normally distributed, the sampling distribution will also be well-behaved and match that math. The “Large Sample” Escape Hatch The statement mentions: “…or if we have a large sample.” This is a reference to the Central Limit Theorem (CLT). In statistics, “Large” usually means a sample size of n&gt;30 (though more is always better). The CLT is like a magic wand: it says that as your sample size grows, the sampling distribution will become normally distributed even if your raw data is messy, skewed, or weird. Why this is a relief: It means you don’t need “perfect” data to get a valid p-value, as long as you have enough participants. How to do it in R: cor(x, y, use = &quot;string&quot;, # everything -&gt; NA for pairs that contain NA # all.obs -&gt; error meassage if there is any NA in the data # complete.obs -&gt; correlations are computed from only cases that are complete for all vars -&gt; more exclusive for NAs # pairwise.complete.obs -&gt; correlations between pairs of bars are computed for cases that are complete for these two vars method = &quot;correlation type&quot; # pearson, spearman, or kendall ) # e.g., cor(df$x, df$y, use = &quot;complete.obs&quot;, method = &quot;pearson&quot;) cor(df[, c(1:3)]) # produce a matrix for three pairs of correlation # other functions cor.test() Hmisc::rcorr(x, y, type = &quot;correlation type&quot;) 5.2.1.1 Interpreting R square Correlation coefficient square, aka coefficient of determination, is the square of r coefficient. It measures the amount of variability of one variable that is shared by the other one. For example, if the coefficient r of bad weather and mood swing is -0.4, the R square will be 0.16. Therefore even though the two vars are nicely correlated, bad weather is still only accounted for 16% of the variation in mood swing. (this is still not an evidence of causality) 5.2.2 Spearman’s rho Same logic as Pearson’s r, but deals with rank data / ordinal data (categories with meaningful order) cor(df$x, df$y, use = &quot;complete.obs&quot;, method = &quot;spearman&quot;) 5.2.3 Kendall’s tau Similar to Spearman’s rho, it deals with non-parametric data, but better suited for cases of small dataset with a large number of tied rank. 5.2.4 Bootstrap correlations More in section 6.5.7 boot(data, function, replications) 5.2.5 Biserial and point-biserial correlations Point-biserial coefficient For discrete dichonomous variable, i.e., binary variable with no continuum in between 0 and 1 (e.g, pregnancy) Biserial coefficient For variable with continuous dichonomy, where an unterlying continuum exisits between 0 and 1 (e.g., pass and fail, where there are many grades in between) library(polycor) polycor::polyserial(df&amp;x, df&amp;y) 5.3 Partial correlation Partial correlation is used when we want to examine the relationship between two variables in which the effect of a third variable is present and held constant. To put it differently, var A accounted for 15% of the variances of var C, var B accounted for 20% of the variances of var C, and there is 5% of variances of var C accounted jointly by var A and B. And then we know that there is 10% of variances of var C accounted uniquely by var A, and 15% of such accounted uniquely by var B. library(ggm) pc &lt;- ggm::pcor(c(A, B, C), df) # in this case we are looking at the unique variances of A accounted by B, with the effect of C held constant pc^2 # R square Semi-partial correlation, aka part correlation, where the third variable’s effect was controlled only for one of the variables in the relationship, and in partial correlation, the effect for both variables in the correlation is controlled. Therefore, partial correlation is used when one wants to look at the unique relationship between two vars when the third var is completely ruled out, and the semi-partial correlation is used when trying to explain the variance in the outcome from a set of predictor variables. (-&gt; see multiple regression?) 5.4 Comparing correlations Independent rs r calculated from different samples, e.g., men and women. We convert these rs to z value to make the smapling distribution normal, and then we compare the value. (see the additional material mentioned in p.239 for more information on calculation in R) Dependent rs see more at page 239 5.5 Computing effect size and reporting results First of all, correlation coefficients are effect sizes. However, the R square of Spearman’s rho needs to be interpreted slighlty different, as they represent the proportion of variances in the ranks that two vars share. See more at 240 Syntax of reporting if you follow the conventions of the American Psychological Association, there should be no zero before the decimal point for the correlation coefficient or the probability value (because neither can exceed 1); coefficients are reported to 2 decimal places; if you are quoting a one-tailed probability, you should say so; each correlation coefficient is represented by a different letter (and some of them are Greek); and there are standard criteria of probabilities that we use (.05, .01 and .001). Example of reporting correlation There was a significant relationship between the number of adverts watched and the number of packets of sweets purchased, r = .87, p (one-tailed) &lt; .05. Exam performance was significantly correlated with exam anxiety, r = −.44, and time spent revising, r = .40; the time spent revising was also correlated with exam anxiety, r = −.71 (all ps &lt; .001). Creativity was significantly related to how well people did in the World’s Biggest Liar competition, rs = −.37, p &lt; .001. Creativity was significantly related to how well people did in the World’s Biggest Liar competition, τ = −.30, p &lt; .001. (Note that I’ve quoted Kendall’s τ here.) The gender of the cat was significantly related to the time the cat spent away from home, rpb = .38, p &lt; .01. The gender of the cat was significantly related to the time the cat spent away from home, rb = .48, p &lt; .01. Q. is r/r square an expression of effect size? Q. how come that squaring of r can be the value to represent the percentage of the variability? Q. why do we need another calculation to compare r? # Regression ## Introduction to regression ## General procedure for regression analysis ## Interpreting simple regression ## Multiple regression basics ## Example studies ### Example study I (Leung and Lee, 2014) - Objective Study the consumption of online alternative media, mainly the drives (predictors) and consequences (impacts) of such consumption. - RQ and hypothesis - Hypothesis 1: Support for democratization relates positively to Internet alternative media usage. - Hypothesis 2: Perception of media self-censorship relates positively to Internet alternative media usage. - Hypothesis 3: News acquisition via Facebook relates positively to Internet alternative media usage. - Hypothesis 4: The relationship stipulated in Hypothesis 3 is stronger among supporters of democratization. - Hypothesis 5: The relationship stipulated in Hypothesis 3 is stronger among people who perceive media self-censorship as serious. - Hypothesis 6: Internet alternative media usage relates positively to protest participation. - Hypothesis 7: Internet alternative media usage relates positively to support for the planned civil disobedience campaign for universal suffrage. - Hypothesis 8: The relationships stipulated in Hypotheses 6 and 7 are stronger among supporters of democratization. The selection of the variables is based on hypotheses, which were traced essentially back to theories. - Method Data: survey data Analysis: 2 hierarchichal regression models targeting different sets of hypos (predictors + impacts). “The regression model contains all the controls, the main effect variables for Hypotheses 1 to 3 (i.e., support for democratization, perceived media self-censorship, and news acquisition via social media), and two interaction terms for testing Hypotheses 4 and 5 (News acquisition via social media × Support for democratization, and News acquisition via social media × Perceived self-censorship). The interaction terms were centered by means to reduce multicollinearity.” (Leung and Lee, 2014, p. 349) “We can now turn to the impact of Internet alternative media usage on protest participation and attitude toward civil disobedience as suggested in Hypotheses 6 to 8. Two hierarchical regression analyses were conducted for the purpose. The independent variables are largely the same as those in Table 1. The only exceptions are the omission of perceived media self-censorship (which is not central to explain protest participation) and that the interaction between news acquisition via social media and perceived media self-censorship is replaced by Internet alternative media usage × Support for democratization, the interaction term central to Hypothesis 8. The interaction between news acquisition via social media and support for democratization is retained for illustrative purposes.” (Leung and Lee, 2014, p. 351) note for further study interaction term as a variable in the regression model, how? why do you need to center the terms by means to reduce multicollinearity? classic imputation strategies for missing values (“missing values were replaced by means”) how to interpret R2 and the triangle R2? citation: Leung, D. K. K., &amp; Lee, F. L. F. (2014). Cultivating an Active Online Counterpublic: Examining Usage and Political Impact of Internet Alternative Media. The International Journal of Press/Politics, 19(3), 340-359. https://doi.org/10.1177/1940161214530787 (Original work published 2014) "],["logistic-regression.html", "Chapter 6 Logistic regression 6.1 Example studies", " Chapter 6 Logistic regression 6.1 Example studies 6.1.1 Example study 1 (Müller &amp; Bach, 2021) “Preliminary analyses revealed that there is an excess number of cases with zero exposure to alternative news in both data sets. To account for this skewed distribution of our criterion variable, we opted to use hurdle models (see Mullahy, 1986) to analyze predictors of alternative news use.” (Müller &amp; Bach, 2021) what is a hurdle model? -&gt; particularly useful for data with many zero values Mullahy J (1986) Specification and testing of some modified count data models. Journal of Econometrics 33(3): 341–365. supplementary materials: https://osf.io/67rwq/overview "],["comparing-two-means-t-test.html", "Chapter 7 Comparing two means (t-test) 7.1 Example studies", " Chapter 7 Comparing two means (t-test) 7.1 Example studies 7.1.1 Example study I (Ohme et al., 2016) Research objective Develope a media exposure tool targeting the high-choice information environment and test the tool in a diary study RQ and hypothesis RQ1a: What is the response rate of our mobile exposure measurement diary in general and how does it differ between age groups? Method “RQ 2 asked whether the difference in platform (app or mobile browser) affects the media exposure measured for the respondents. Three t-tests were conducted, testing for differences between respondents using the two platforms. The first independent group t-test revealed that respondents taking the survey in their mobile browser (M = .42, SD = .29) were slightly but significantly exposed to more offline political information than respondents using the app (M = .38, SD = .28); t (1648) = 3.8, p &lt; .000. The second t-test revealed no significant difference in political online media exposure (t (2376) = −1.8, p &gt; .05) between respondents using the app (M = .15, SD = .15) and respondents using the mobile browser (M = .14, SD = .16). Likewise, the third t-test showed no significant difference in exposure to political information on social media (t(2376) = 1.5, p &gt; .05) between respondents using the app (M = .20, SD = .25) and respondents participating via a mobile browser (M = .22, SD = .30). Thus, we only observed a small skew for one of three media exposure types, indicating considerable congruency between platform modes.” (Ohme et al., 2016, p.141 - 142) note. what is this t (2376) reporting? citation: Ohme, J., Albaek, E., &amp; H. de Vreese, C. (2016). Exposure Research Going Mobile: A Smartphone-Based Measurement of Media Exposure to Political Information in a Convergent Media Environment. Communication Methods and Measures, 10(2–3), 135–148. https://doi.org/10.1080/19312458.2016.1150972 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
